{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 総称としての勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データと予測値のコスト関数を微分し、勾配(gradient)を利用することによって、訓練データと予測値の誤差を小さくしていく方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ・(バッチ)勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全ての訓練データと予測値の誤差の和をとり、勾配を使ってその和が最小となるようなパラメータを求める方法。\n",
    "\n",
    "ここでは、平均二乗誤差$ MSE $の最小化を目指すとする。$MSE$を、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    MSE(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}({\\theta}^T \\cdot x^{(i)} - y^{(i)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とすれば、パラメータ$\\theta_i$についてのコスト関数$MSE$の偏微分は、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\frac{\\partial}{\\partial \\theta_j} MSE(\\theta) = \\frac{2}{m}\\sum_{i=1}^{m}({\\theta}^T \\cdot x^{(i)} - y^{(i)}){x_j}^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となるから、このコスト関数の勾配ベクトルは、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\nabla_{\\theta}MSE(\\theta) = \\left(\n",
    "    \\begin{array}{cccc}\n",
    "      \\frac{\\partial}{\\partial \\theta_0} MSE(\\theta)  \\\\\n",
    "      \\frac{\\partial}{\\partial \\theta_1} MSE(\\theta)  \\\\\n",
    "      \\vdots   \\\\\n",
    "      \\frac{\\partial}{\\partial \\theta_n} MSE(\\theta) \n",
    "    \\end{array}\n",
    "  \\right) = \\frac{2}{m}X^T \\cdot (X \\cdot \\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となる。\n",
    "\n",
    "この勾配ベクトルに、学習率$\\eta$(ステップサイズ)をかけたものをパラメータ$\\theta$から引くことによって$\\theta$を更新していく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\theta_{n+1}= \\theta_n - \\eta \\nabla_{\\theta}MSE(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ・確率的勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチ勾配降下法では各ステップですべての訓練データと予測値の誤差を計算していたが、\n",
    "\n",
    "この方法だと訓練データが多くなればなるほど計算コストが大きくなってしまう。\n",
    "\n",
    "確率的勾配降下法は、各ステップで訓練データからランダムな1つのデータのみを選び、そのコスト関数Cから勾配を計算していく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    C(\\theta) = ({\\theta}^T \\cdot x^{(i)} - y^{(i)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\vdots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算に使う訓練データの数が変わるだけで他は同じ。\n",
    "\n",
    "ランダムにとってきたデータを使うので、バッチ勾配降下法に比べて局所解に陥る可能性が低い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ・ミニバッチ勾配降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチ勾配降下法と確率的勾配降下法を合わせたような手法で、\n",
    "\n",
    "ハイパーパラメータとしてバッチサイズを決め、訓練データからバッチサイズ分だけデータをとってきて勾配を計算していく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ・まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- バッチ勾配降下法を基に確率的勾配降下法やミニバッチ勾配降下法が考えられた\n",
    "- ミニバッチ勾配降下法が最も収束が早くなる場合が多いらしい\n",
    "- パラメータの更新方法が上記以外にも様々あるようなので、どの最適化が収束が早いかもそれによっても変わる？\n",
    "- (パラメータの更新方法によって？)学習率をハイパーパラメータにするかどうか変わるようなので追加で調査する"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
